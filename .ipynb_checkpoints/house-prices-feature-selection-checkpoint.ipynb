{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "In this notebook we compile feature engineering ideas found in other notebooks and from the [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) course. We then use techniques to try to determine which features are the most important. In particular, we will consider the following methods:\n",
    "\n",
    "1. Mutual Information Scores\n",
    "2. F Scores\n",
    "3. Permutation Importance\n",
    "\n",
    "In a future version, I will extend the notebook to consider model-based feature importances and SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:03.777331Z",
     "iopub.status.busy": "2022-09-16T05:19:03.776814Z",
     "iopub.status.idle": "2022-09-16T05:19:05.159778Z",
     "shell.execute_reply": "2022-09-16T05:19:05.158673Z",
     "shell.execute_reply.started": "2022-09-16T05:19:03.777236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "NUM_FOLDS = 12\n",
    "RANDOM_SEED = 10\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Speedup some scikit-learn algorithms\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "import sklearn\n",
    "\n",
    "# Preprocessing Imports\n",
    "from sklearn.base import clone\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer, FunctionTransformer, QuantileTransformer\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, boxcox_normmax, norm\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "# Original Data\n",
    "original_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "original_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col = 'Id')\n",
    "\n",
    "# Preprocessed Data\n",
    "train = pd.read_csv('../input/house-prices-ames-cleaned-dataset/new_train.csv')\n",
    "test = pd.read_csv('../input/house-prices-ames-cleaned-dataset/new_test.csv', index_col = 'Id')\n",
    "submission = pd.read_csv('../input/house-prices-ames-cleaned-dataset/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering\n",
    "\n",
    "These features are taken from public notebooks and the Kaggle learn course. On their own, we don't know the value each feature brings to our model (if any). We will use several techniques to evaluate the feature importance in successive sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:05.162218Z",
     "iopub.status.busy": "2022-09-16T05:19:05.161885Z",
     "iopub.status.idle": "2022-09-16T05:19:05.394048Z",
     "shell.execute_reply": "2022-09-16T05:19:05.39303Z",
     "shell.execute_reply.started": "2022-09-16T05:19:05.162186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill NAs with the median of the Neighborhood\n",
    "train[\"LotFrontage\"] = train[[\"LotFrontage\",\"Neighborhood\"]].groupby(\"Neighborhood\").transform(lambda x: x.fillna(x.median()))\n",
    "test[\"LotFrontage\"] = test[[\"LotFrontage\",\"Neighborhood\"]].groupby(\"Neighborhood\").transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Has MiscFeature\n",
    "train['MiscFeature'] = (train['MiscFeature'].notna()).astype(int)\n",
    "test['MiscFeature'] = (test['MiscFeature'].notna()).astype(int)\n",
    "\n",
    "# Has Garage\n",
    "train['HasGarage'] = (train['GarageType'].notna()).astype(int)\n",
    "test['HasGarage'] = (test['GarageType'].notna()).astype(int)\n",
    "\n",
    "# Has Pool\n",
    "train['HasPool'] = (train['PoolArea'] > 0).astype(int)\n",
    "test['HasPool'] = (test['PoolArea'] > 0).astype(int)\n",
    "train.drop(columns = 'PoolQC', inplace = True)\n",
    "test.drop(columns = 'PoolQC', inplace = True)\n",
    "\n",
    "# Has Porch\n",
    "porch_cols = ['OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch']\n",
    "train['HasPorch'] = (train[porch_cols].sum(axis = 1) > 0).astype(int)\n",
    "test['HasPorch'] = (test[porch_cols].sum(axis = 1) > 0).astype(int)\n",
    "\n",
    "# Has Fireplace\n",
    "train['HasFireplace'] = (train['Fireplaces'] > 0).astype(int)\n",
    "test['HasFireplace'] = (test['Fireplaces'] > 0).astype(int)\n",
    "\n",
    "# Has Fence\n",
    "train['HasFence'] = (train['Fence'] > 0).astype(int)\n",
    "test['HasFence'] = (test['Fence'] > 0).astype(int)\n",
    "\n",
    "# Has Veneer\n",
    "train['HasVeneer'] = (train['MasVnrType'].notna()).astype(int)\n",
    "test['HasVeneer'] = (test['MasVnrType'].notna()).astype(int)\n",
    "\n",
    "# Has 2nd Floor\n",
    "train['Has2ndFloor'] = (train['2ndFlrSF'] > 0).astype(int)\n",
    "test['Has2ndFloor'] = (test['2ndFlrSF'] > 0).astype(int)\n",
    "\n",
    "# Has Basement\n",
    "train['HasBasement'] = (train['BsmtCond'] > 0).astype(int)\n",
    "test['HasBasement'] = (test['BsmtCond'] > 0).astype(int)\n",
    "\n",
    "# Was Remodeled\n",
    "train['Remodel'] = (train['YearRemodAdd'] != train['YearBuilt']).astype(int)\n",
    "test['Remodel'] = (test['YearRemodAdd'] != test['YearBuilt']).astype(int)\n",
    "\n",
    "# Age\n",
    "train['House_Age'] = train['YrSold'] - train['YearBuilt']\n",
    "test['House_Age'] = test['YrSold'] - test['YearBuilt']\n",
    "\n",
    "# Total Square Footage\n",
    "train['TotalSF'] = train[['TotalBsmtSF','GrLivArea']].sum(axis = 1)\n",
    "test['TotalSF'] = test[['TotalBsmtSF','GrLivArea']].sum(axis = 1)\n",
    "\n",
    "# Total bathrooms\n",
    "train['TotalBath'] = train[['FullBath','BsmtFullBath']].sum(axis = 1) + 0.5 * train[['HalfBath','BsmtHalfBath']].sum(axis = 1)\n",
    "test['TotalBath'] = test[['FullBath','BsmtFullBath']].sum(axis = 1) + 0.5 * test[['HalfBath','BsmtHalfBath']].sum(axis = 1)\n",
    "\n",
    "# Total Porch/Deck space\n",
    "train['TotalPorch'] = train[porch_cols].sum(axis = 1)\n",
    "test['TotalPorch'] = test[porch_cols].sum(axis = 1)\n",
    "\n",
    "# Ratio of living space to Lot Size\n",
    "train[\"LivLotRatio\"] = train[\"GrLivArea\"] / train[\"LotArea\"]\n",
    "test[\"LivLotRatio\"] = test[\"GrLivArea\"] / test[\"LotArea\"]\n",
    "\n",
    "# Avg square feet per room\n",
    "train[\"Spaciousness\"] = (train[\"1stFlrSF\"]+train[\"2ndFlrSF\"]) / train[\"TotRmsAbvGrd\"]\n",
    "test[\"Spaciousness\"] = (test[\"1stFlrSF\"]+test[\"2ndFlrSF\"]) / test[\"TotRmsAbvGrd\"]\n",
    "\n",
    "# Lot space + frontage\n",
    "train['TotalLot'] = train['LotFrontage'] + train['LotArea']\n",
    "test['TotalLot'] = test['LotFrontage'] + test['LotArea']\n",
    "\n",
    "# Total finished basement\n",
    "train['TotalBsmtFin'] = train['BsmtFinSF1'] + train['BsmtFinSF2']\n",
    "test['TotalBsmtFin'] = test['BsmtFinSF1'] + test['BsmtFinSF2']\n",
    "\n",
    "# PCA inspired feature \n",
    "train[\"PCA_Feature1\"] = train['GrLivArea'] + train['TotalBsmtSF']\n",
    "test[\"PCA_Feature1\"] = test['GrLivArea'] + test['TotalBsmtSF']\n",
    "\n",
    "# PCA inspired feature\n",
    "train[\"PCA_Feature2\"] = train['YearRemodAdd'] * train['TotalBsmtSF']\n",
    "test[\"PCA_Feature2\"] = test['YearRemodAdd'] * test['TotalBsmtSF']\n",
    "\n",
    "# Merging quality and conditions.\n",
    "\n",
    "train['TotalQual'] = train[['OverallQual','OverallCond']].sum(axis = 1)\n",
    "test['TotalQual'] = test[['OverallQual','OverallCond']].sum(axis = 1)\n",
    "\n",
    "train['TotalExtQual'] = train[['ExterQual','ExterCond']].sum(axis = 1)\n",
    "test['TotalExtQual'] = test[['ExterQual','ExterCond']].sum(axis = 1)\n",
    "\n",
    "train['TotalBsmtQual'] = train[['BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2']].sum(axis = 1)\n",
    "test['TotalBsmtQual'] = test[['BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2']].sum(axis = 1)\n",
    "\n",
    "train['TotalGrgQual'] = train[['GarageQual','GarageCond']].sum(axis = 1)\n",
    "test['TotalGrgQual'] = test[['GarageQual','GarageCond']].sum(axis = 1)\n",
    "\n",
    "train['TotalMiscQual'] = train[['KitchenQual','HeatingQC']].sum(axis = 1)\n",
    "test['TotalMiscQual'] = test[['KitchenQual','HeatingQC']].sum(axis = 1)\n",
    "\n",
    "# Interaction Features w/ Qual columns\n",
    "\n",
    "train['QualGrLiv'] = train['TotalQual'] * train['GrLivArea']\n",
    "test['QualGrLiv'] = test['TotalQual'] * test['GrLivArea']\n",
    "\n",
    "train['QualBsmtArea'] = train['TotalBsmtQual'] * train['TotalBsmtFin']\n",
    "test['QualBsmtArea'] = test['TotalBsmtQual'] * test['TotalBsmtFin']\n",
    "\n",
    "train['QualPorchSF'] = train['TotalExtQual'] * train['TotalPorch']\n",
    "test['QualPorchSF'] = test['TotalExtQual'] * test['TotalPorch']\n",
    "\n",
    "train['QualExt'] = train['TotalExtQual'] * train['MasVnrArea']\n",
    "test['QualExt'] = test['TotalExtQual'] * test['MasVnrArea']\n",
    "\n",
    "train['QualGrg'] = train['TotalGrgQual'] * train['GarageArea']\n",
    "test['QualGrg'] = test['TotalGrgQual'] * test['GarageArea']\n",
    "\n",
    "# Neighborhood wealth 0: cheap, 1: typical, 2: expensive\n",
    "mapping = {\n",
    "    'StoneBr':2, 'NridgHt':2, 'NoRidge':2, \n",
    "    'CollgCr':1, 'Veenker':1, 'Crawfor':1, 'Mitchel':1, 'Somerst':1,\n",
    "    'NWAmes':1, 'OldTown':1, 'BrkSide':1, 'Sawyer':1, 'NAmes':1,\n",
    "    'SawyerW':1, 'Edwards':1, 'Timber':1, 'Gilbert':1,\n",
    "    'ClearCr':1, 'NPkVill':1, 'Blmngtn':1, 'SWISU':1, 'Blueste':1,\n",
    "    'MeadowV':0, 'IDOTRR':0, 'BrDale':0\n",
    "}\n",
    "train['NbhdCost'] = train['Neighborhood'].replace(mapping)\n",
    "test['NbhdCost'] = test['Neighborhood'].replace(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pipelines\n",
    "\n",
    "We create functions which take a selection of columns and output a scikit-learn pipeline which we can use to test our feature selection. For simplicity we consider two models:\n",
    "\n",
    "1. Gradient Boosting\n",
    "2. Linear Model w/ Regularization\n",
    "\n",
    "We avoid L1 regularization since we want all of our feature selection to come from our manual feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:05.395816Z",
     "iopub.status.busy": "2022-09-16T05:19:05.395473Z",
     "iopub.status.idle": "2022-09-16T05:19:05.409338Z",
     "shell.execute_reply": "2022-09-16T05:19:05.408179Z",
     "shell.execute_reply.started": "2022-09-16T05:19:05.395783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our cross-validation splits\n",
    "skf = list(StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED).split(train[test.columns], train['Neighborhood']))\n",
    "\n",
    "# Data structure for saving RMSE scores\n",
    "cv_scores = {'Scheme': [f'Fold {i}' for i in range(NUM_FOLDS)] + ['Best','Median','Average','Worst']}\n",
    "\n",
    "# Function for displaying scores\n",
    "def sort_scores(scores):\n",
    "    temp = pd.DataFrame(scores)\n",
    "    temp.set_index('Scheme', inplace = True)\n",
    "    temp = temp.T\n",
    "    return temp.sort_values('Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Gradient Boosting Pipeline\n",
    "\n",
    "We train a gradient boosting model using the HistGradientBoostingRegressor from scikit-learn. We let the model use built-in categorical feature handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:05.413555Z",
     "iopub.status.busy": "2022-09-16T05:19:05.412468Z",
     "iopub.status.idle": "2022-09-16T05:19:05.420043Z",
     "shell.execute_reply": "2022-09-16T05:19:05.418662Z",
     "shell.execute_reply.started": "2022-09-16T05:19:05.413516Z"
    }
   },
   "outputs": [],
   "source": [
    "def gb_pipeline(features):\n",
    "    \n",
    "    # Categorical Features\n",
    "    categorical = [x for x in features if test[x].dtype == 'object']\n",
    "    categorical_mask = [True]*len(categorical) + [False]*(len(features) - len(categorical))\n",
    "\n",
    "    # Full Pipeline\n",
    "    return make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (OrdinalEncoder(), categorical), \n",
    "            remainder = SimpleImputer(strategy = 'median')\n",
    "        ),\n",
    "        HistGradientBoostingRegressor(categorical_features = categorical_mask)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Linear Pipeline\n",
    "\n",
    "We train a linear regression model with l2 regularization (aka Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:05.421816Z",
     "iopub.status.busy": "2022-09-16T05:19:05.421491Z",
     "iopub.status.idle": "2022-09-16T05:19:05.436624Z",
     "shell.execute_reply": "2022-09-16T05:19:05.435643Z",
     "shell.execute_reply.started": "2022-09-16T05:19:05.421786Z"
    }
   },
   "outputs": [],
   "source": [
    "def lm_pipeline(features):\n",
    "    \n",
    "    categorical = [x for x in features if test[x].dtype == 'object']\n",
    "    numerical = [x for x in features if x not in categorical]\n",
    "    temp = train[numerical].skew()\n",
    "    skewed = list(temp[temp > 0.7].index)\n",
    "    \n",
    "    return make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (OneHotEncoder(), categorical),\n",
    "            (FunctionTransformer(np.log1p), skewed),\n",
    "            remainder = 'passthrough' \n",
    "        ),\n",
    "        SimpleImputer(),\n",
    "        RobustScaler(),\n",
    "        Ridge(alpha = 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:05.438984Z",
     "iopub.status.busy": "2022-09-16T05:19:05.438263Z",
     "iopub.status.idle": "2022-09-16T05:19:23.432703Z",
     "shell.execute_reply": "2022-09-16T05:19:23.431161Z",
     "shell.execute_reply.started": "2022-09-16T05:19:05.438948Z"
    }
   },
   "outputs": [],
   "source": [
    "# All Features\n",
    "features = test.columns\n",
    "\n",
    "# Gradient Boosting \n",
    "scores = cross_validate(\n",
    "    estimator = gb_pipeline(features),\n",
    "    X = train[features],\n",
    "    y = np.log1p(train['SalePrice']),\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = skf,\n",
    ")\n",
    "\n",
    "# Save Scores\n",
    "cv_scores['GB (all)'] = np.concatenate(\n",
    "    (-scores['test_score'],-np.max(scores['test_score']), -np.mean(scores['test_score']), -np.median(scores['test_score']), -np.min(scores['test_score'])), axis = None\n",
    ")\n",
    "\n",
    "# Ridge Regressor\n",
    "scores = cross_validate(\n",
    "    estimator = lm_pipeline(features),\n",
    "    X = train[features],\n",
    "    y = np.log1p(train['SalePrice']),\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = skf,\n",
    ")\n",
    "\n",
    "# Save Scores\n",
    "cv_scores['LM (all)'] = np.concatenate(\n",
    "    (-scores['test_score'],-np.max(scores['test_score']), -np.mean(scores['test_score']), -np.median(scores['test_score']), -np.min(scores['test_score'])), axis = None\n",
    ")\n",
    "\n",
    "sort_scores(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mutual Information\n",
    "\n",
    "1. Encode categorical features using ordinal encoding\n",
    "2. Calculate mutual information (indicating which features are categorical)\n",
    "3. Remove features under a given threshold\n",
    "4. Evaluate model with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:23.44483Z",
     "iopub.status.busy": "2022-09-16T05:19:23.440626Z",
     "iopub.status.idle": "2022-09-16T05:19:23.467401Z",
     "shell.execute_reply": "2022-09-16T05:19:23.46569Z",
     "shell.execute_reply.started": "2022-09-16T05:19:23.444748Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop columns with mutual info score < threshold\n",
    "def high_mi_columns(threshold = 0.01):\n",
    "    \n",
    "    # encode categorical variables\n",
    "    features = test.columns\n",
    "    categorical = [x for x in features if test[x].dtype == 'object'] \n",
    "    new_train = OrdinalEncoder(cols = categorical).fit_transform(train)\n",
    "\n",
    "    mi_scores = mutual_info_regression(\n",
    "        X = new_train[features], \n",
    "        y = np.log1p(train['SalePrice']), \n",
    "        discrete_features = [x in categorical for x in features], \n",
    "        random_state = RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    temp = pd.Series(data = mi_scores,index = features).sort_values(ascending = False)\n",
    "    features = list(temp[temp > threshold].index)\n",
    "    print(\"Dropped:\", *temp[temp <= threshold].index, '\\n')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:23.470752Z",
     "iopub.status.busy": "2022-09-16T05:19:23.469905Z",
     "iopub.status.idle": "2022-09-16T05:19:39.413666Z",
     "shell.execute_reply": "2022-09-16T05:19:39.409898Z",
     "shell.execute_reply.started": "2022-09-16T05:19:23.470704Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "columns = high_mi_columns()\n",
    "\n",
    "# HistGradientBoostingRegressor\n",
    "scores = cross_validate(\n",
    "    estimator = gb_pipeline(columns),\n",
    "    X = train[columns],\n",
    "    y = np.log1p(train['SalePrice']),\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = skf,\n",
    ")\n",
    "\n",
    "# Save Scores\n",
    "cv_scores['GB (MI)'] = np.concatenate(\n",
    "    (-scores['test_score'],-np.max(scores['test_score']), -np.mean(scores['test_score']), -np.median(scores['test_score']), -np.min(scores['test_score'])), axis = None\n",
    ")\n",
    "\n",
    "# Ridge Regressor\n",
    "scores = cross_validate(\n",
    "    estimator = lm_pipeline(columns),\n",
    "    X = train[columns],\n",
    "    y = np.log1p(train['SalePrice']),\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = skf,\n",
    ")\n",
    "\n",
    "# Save Scores\n",
    "cv_scores['LM (MI)'] = np.concatenate(\n",
    "    (-scores['test_score'],-np.max(scores['test_score']), -np.mean(scores['test_score']), -np.median(scores['test_score']), -np.min(scores['test_score'])), axis = None\n",
    ")\n",
    "\n",
    "sort_scores(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. F-Score\n",
    "\n",
    "1. Encode categorical features using one-hot encoding\n",
    "2. Get F-scores for each feature\n",
    "3. Remove features under a given threshold\n",
    "4. Evaluate model with removed features\n",
    "\n",
    "Currently, we drop categorical features if ALL of the categories score less than the threshold. In a later version, I intend to rewrite this so that only the categories scoring below the threshold are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:39.416983Z",
     "iopub.status.busy": "2022-09-16T05:19:39.416276Z",
     "iopub.status.idle": "2022-09-16T05:19:39.441698Z",
     "shell.execute_reply": "2022-09-16T05:19:39.439228Z",
     "shell.execute_reply.started": "2022-09-16T05:19:39.4168Z"
    }
   },
   "outputs": [],
   "source": [
    "def high_fscore_columns(threshold = 5):\n",
    "    \n",
    "    features = test.columns\n",
    "    categorical = [x for x in features if test[x].dtype == 'object']\n",
    "    numerical = [x for x in features if x not in categorical]\n",
    "\n",
    "    # encode categorical variables\n",
    "    new_train = OneHotEncoder(\n",
    "        cols = categorical,\n",
    "        use_cat_names = True\n",
    "    ).fit_transform(train[features])\n",
    "\n",
    "\n",
    "    f_scores, p_values = f_regression(\n",
    "        X = new_train, \n",
    "        y = np.log1p(train['SalePrice']), \n",
    "    )\n",
    "\n",
    "    # for categorical variables, all related column have to be below the threshold\n",
    "    f_series = pd.Series(data = f_scores, index = new_train.columns).sort_values(ascending = False)\n",
    "    f_dict = f_series.to_dict()\n",
    "    f_cat = pd.Series(\n",
    "        {col: np.max([f_dict[x] for x in f_dict.keys() if x.startswith(col)]) for col in categorical}\n",
    "    ).sort_values(ascending = False)\n",
    "    f_num = pd.Series({x: f_dict[x] for x in numerical}).sort_values(ascending = False)\n",
    "\n",
    "    f_features = list(f_cat[f_cat > threshold ].index) + list(f_num[f_num > threshold ].index)\n",
    "    f_dropped = list(f_cat[f_cat <= threshold ].index) + list(f_num[f_num <= threshold ].index)\n",
    "    print(\"Dropped:\", *f_dropped, '\\n')\n",
    "    return f_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:39.450062Z",
     "iopub.status.busy": "2022-09-16T05:19:39.448979Z",
     "iopub.status.idle": "2022-09-16T05:19:55.109266Z",
     "shell.execute_reply": "2022-09-16T05:19:55.107732Z",
     "shell.execute_reply.started": "2022-09-16T05:19:39.449998Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "columns = high_fscore_columns()\n",
    "\n",
    "# HistGradientBoostingRegressor\n",
    "scores = cross_validate(\n",
    "    estimator = gb_pipeline(columns),\n",
    "    X = train[columns],\n",
    "    y = np.log1p(train['SalePrice']),\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = skf,\n",
    ")\n",
    "\n",
    "# Save Scores\n",
    "cv_scores['GB (F)'] = np.concatenate(\n",
    "    (-scores['test_score'],-np.max(scores['test_score']), -np.mean(scores['test_score']), -np.median(scores['test_score']), -np.min(scores['test_score'])), axis = None\n",
    ")\n",
    "\n",
    "# Ridge Regressor\n",
    "scores = cross_validate(\n",
    "    estimator = lm_pipeline(columns),\n",
    "    X = train[columns],\n",
    "    y = np.log1p(train['SalePrice']),\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = skf,\n",
    ")\n",
    "\n",
    "# Save Scores\n",
    "cv_scores['LM (F)'] = np.concatenate(\n",
    "    (-scores['test_score'],-np.max(scores['test_score']), -np.mean(scores['test_score']), -np.median(scores['test_score']), -np.min(scores['test_score'])), axis = None\n",
    ")\n",
    "\n",
    "sort_scores(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Permutation Importance\n",
    "\n",
    "Finally, we drop feature based on feature importance.\n",
    "\n",
    "1. Fit several models using cross-validation\n",
    "2. Evaluate permutation importance on each fold\n",
    "3. Average feature importance over all folds\n",
    "4. Remove features under a given threshold\n",
    "5. Evaluate model with removed features\n",
    "\n",
    "Unlike the previous two methods, permutation importance is based on evaluating the model on multiple permutations of various columns. Hence, we have to run it separately for both models. This can take way longer than the other two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:55.112812Z",
     "iopub.status.busy": "2022-09-16T05:19:55.111722Z",
     "iopub.status.idle": "2022-09-16T05:19:55.133996Z",
     "shell.execute_reply": "2022-09-16T05:19:55.132311Z",
     "shell.execute_reply.started": "2022-09-16T05:19:55.112753Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_perm_columns(pipeline, threshold = 1e-4):\n",
    "    features = test.columns\n",
    "\n",
    "    scores = cross_validate(\n",
    "        estimator = clone(pipeline),\n",
    "        X = train[features],\n",
    "        y = np.log1p(train['SalePrice']),\n",
    "        scoring = 'neg_root_mean_squared_error',\n",
    "        cv = skf,\n",
    "        return_estimator = True\n",
    "    )\n",
    "\n",
    "    data = dict()\n",
    "    for fold, (model, (train_idx, valid_idx)) in enumerate(zip(scores['estimator'], skf)):\n",
    "        data[fold] = permutation_importance(\n",
    "            estimator = model, \n",
    "            X = train[features].loc[valid_idx],\n",
    "            y = np.log1p(train['SalePrice'].loc[valid_idx]),\n",
    "            scoring = 'neg_root_mean_squared_error',\n",
    "            random_state = RANDOM_SEED\n",
    "        )['importances_mean']\n",
    "\n",
    "    temp = pd.DataFrame(data, index = features).mean(axis = 1).sort_values(ascending = False)\n",
    "    perm_features = temp[temp > threshold].index\n",
    "    p_dropped = set(temp[temp <= threshold].index)\n",
    "    print(\"Dropped:\", *p_dropped, '\\n')\n",
    "    return perm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-09-16T05:19:55.136945Z",
     "iopub.status.busy": "2022-09-16T05:19:55.136025Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perm Importance GB\n",
    "columns = get_perm_columns(gb_pipeline(test.columns))\n",
    "\n",
    "# HistGradientBoostingRegressor\n",
    "scores = cross_validate(\n",
    "    estimator = gb_pipeline(columns),\n",
    "    X = train[columns],\n",
    "    y = np.log1p(train['SalePrice']),\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = skf,\n",
    ")\n",
    "\n",
    "# Save Scores\n",
    "cv_scores['GB (Perm)'] = np.concatenate(\n",
    "    (-scores['test_score'],-np.max(scores['test_score']), -np.mean(scores['test_score']), -np.median(scores['test_score']), -np.min(scores['test_score'])), axis = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perm Importance LM\n",
    "columns = get_perm_columns(lm_pipeline(test.columns))\n",
    "\n",
    "# Ridge Regressor\n",
    "scores = cross_validate(\n",
    "    estimator = lm_pipeline(columns),\n",
    "    X = train[columns],\n",
    "    y = np.log1p(train['SalePrice']),\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = skf,\n",
    ")\n",
    "\n",
    "# Save Scores\n",
    "cv_scores['LM (Perm)'] = np.concatenate(\n",
    "    (-scores['test_score'],-np.max(scores['test_score']), -np.mean(scores['test_score']), -np.median(scores['test_score']), -np.min(scores['test_score'])), axis = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "sort_scores(cv_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
